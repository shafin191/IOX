{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c52178-6751-4e2f-89f6-6b69a3e503f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import torch\n",
    "import open_clip\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import GPUtil\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc20b9b-525a-4ffa-8ab8-5f145ac9a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"\\nGPU {gpu.id}:\")\n",
    "    print(f\"  Name: {gpu.name}\")\n",
    "    print(f\"  Total Memory: {gpu.memoryTotal} MB\")\n",
    "    print(f\"  Memory Used: {gpu.memoryUsed} MB\")\n",
    "    print(f\"  Memory Free: {gpu.memoryFree} MB\")\n",
    "    print(f\"  Temperature: {gpu.temperature} C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6153db-2602-4cb3-b4de-482cf8d990d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a18966-2200-473b-82e5-17285603fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_percent = psutil.cpu_percent(interval=1) \n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {ram.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used RAM: {ram.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available RAM: {ram.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"RAM Usage: {ram.percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ce9af-2d45-4dbe-adcd-1c427d688180",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor()            \n",
    "])\n",
    "\n",
    "def get_all_image_paths(directory):\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}  \n",
    "    image_paths = []\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def process_images(image_paths, batch_size=64):\n",
    "    all_embeddings_image = []\n",
    "    all_filenames = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Processing Images\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")  \n",
    "                batch_images.append(image)  \n",
    "                all_filenames.append(os.path.basename(img_path))  \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "        if batch_images:\n",
    "            with torch.no_grad():  \n",
    "                inputs = processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "                embeddings = model.get_image_features(pixel_values=inputs.pixel_values)  \n",
    "            all_embeddings_image.append(embeddings.cpu().numpy())  \n",
    "\n",
    "    return np.vstack(all_embeddings_image), all_filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a09fd-a184-4e08-b1c4-5aabecb73275",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"\"\n",
    "image_paths = get_all_image_paths(image_dir)\n",
    "print(f\"Found {len(image_paths)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f4834-cb86-40ae-8fac-e15b27f5d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = image_dir\n",
    "subdirectories = os.listdir(main_directory)  \n",
    "pickle_file_location = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301a975-dcec-4b5f-80b5-66b10986b754",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for path in tqdm(subdirectories, desc=\"Processing User IDs\", unit=\"user\"):\n",
    "    print(f\"User_ID : {path}\")\n",
    "    start = time.time()\n",
    "    image_folder = os.path.join(main_directory, path) \n",
    "    image_paths = glob(os.path.join(image_folder, \"*.jpg\"))\n",
    "    valid_image_paths = [img_path for img_path in image_paths if os.path.getsize(img_path) > 0]\n",
    "    try:\n",
    "        features = process_images(valid_image_paths)\n",
    "        dirc = Path(pickle_file_location + path)\n",
    "        dirc.mkdir(parents=True, exist_ok=True)\n",
    "        pd.concat([pd.DataFrame(features[1]).rename(columns = {0:'Tweet_ID'}),pd.DataFrame(features[0])], axis = 1).to_pickle(pickle_file_location + path + \"/Image_Embedding_\" + path + \".pkl\")\n",
    "        end = time.time()\n",
    "        print(f\"Total time taken: {end - start:.2f} seconds\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8545a1b-6b5c-49dc-8533-4f455aaa82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickles_from_directory(root_dir):\n",
    "    all_data = []\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        user_id = os.path.basename(subdir)  \n",
    "        for file in files:\n",
    "            if file.endswith('.pkl'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                        if isinstance(data, pd.DataFrame):\n",
    "                            data['User_ID'] = user_id \n",
    "                            if 'Tweet_ID' in data.columns:\n",
    "                                data['Tweet_ID'] = data['Tweet_ID'].str.replace('.jpg', '', regex=False)\n",
    "                            all_data.append(data)\n",
    "                        else:\n",
    "                            print(f\"Skipping {file_path}: Not a DataFrame\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")   \n",
    "    if all_data:\n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356265cc-8a0c-427e-96ee-e45e0420148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"\"  \n",
    "merged_df = load_pickles_from_directory(root_directory)\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d919f3b-7011-413f-814e-5e02a4448020",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_pickle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787dad4-6135-42e1-818d-548f08972957",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27f9d5-09d2-44e8-b0ef-e14b5ba13e0c",
   "metadata": {},
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246219d-e486-4d88-b048-5e5031db8410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import sys\n",
    "import GPUtil\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a214a3f-f458-4d99-a3de-fa7d6506da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(faiss.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8fced-dde4-47e4-899e-e73b1aaf85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df9432-166e-4625-9441-88801ef0c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['Tweet_ID', 'User_ID']].reset_index().rename(columns = {'index': 'Text_Index'}).to_csv('Politix_Image_Index_Tweet_ID_User_ID_Mapping_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba0a99-bfd5-454a-9888-1a05fdcdcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index = merged_df[['Tweet_ID', 'User_ID']].reset_index().rename(columns = {'index': 'Text_Index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c416ece-015d-4da8-822b-021a19216fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb6e77-ad63-47a0-8116-78030dc62224",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_arr = np.array(merged_df.iloc[:,1:-1])\n",
    "f_arr = f_arr.astype(\"float32\") \n",
    "f_arr = np.ascontiguousarray(f_arr)  \n",
    "faiss.normalize_L2(f_arr)  \n",
    "dim = f_arr.shape[1]  \n",
    "cpu_index = faiss.IndexFlatIP(dim)  \n",
    "cpu_index.add(f_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d3a70-bac7-4ef6-a7fa-dce38d08fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941cfae-5960-4672-bb12-865de232a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits, distances, indices = cpu_index.range_search(f_arr, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efccd73-e7aa-48df-a8a5-a205e5fdc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs_dict = {}\n",
    "seen_pairs = set()  \n",
    "\n",
    "for i in range(len(limits) - 1):\n",
    "    start, end = limits[i], limits[i + 1]\n",
    "    \n",
    "    for j in range(start, end):\n",
    "        a, b = i, indices[j]\n",
    "        if a != b: \n",
    "            pair = (min(a, b), max(a, b))\n",
    "            if pair not in seen_pairs:\n",
    "                seen_pairs.add(pair)\n",
    "                if a not in similar_pairs_dict:\n",
    "                    similar_pairs_dict[a] = []\n",
    "                similar_pairs_dict[a].append({\"neighbor\": b, \"distance\": distances[j]})\n",
    "\n",
    "df_res = pd.DataFrame([\n",
    "    {\"source\": key, \"neighbor\": entry[\"neighbor\"], \"distance\": entry[\"distance\"]}\n",
    "    for key, values in similar_pairs_dict.items() for entry in values\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3efae-16db-466e-8af7-972c450c0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_pickle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a0e44-9285-48a6-b197-174f946112f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.loc[df_res.distance>= 0.99].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f3a51-c7b4-47bc-aab5-243aef8ddbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.loc[(df_res.distance<= 0.99) & (df_res.distance>= 0.98)].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0feaf73-e894-47f4-91f0-3e379ba49988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f71912-dec2-49b0-9d69-8b1364192db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93656b4-c9a3-44d2-a9f8-55a34c8cefb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
