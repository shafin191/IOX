{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d32105-393c-468a-b2cd-a7069d3edee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334a511b-d317-42b3-964b-ecf30ea9f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n",
      "Model is ready on: cuda:3\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check GPU availability\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)  # Distribute model across all GPUs\n",
    "\n",
    "# Move model to CUDA\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Put model in inference mode\n",
    "\n",
    "# Optional: Use Torch Compile for Speedup (Only for PyTorch 2.0+)\n",
    "# model = torch.compile(model)\n",
    "\n",
    "print(\"Model is ready on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb3c8b5-9f09-4a91-88a5-7da38e6a405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/a/bear.cs.xxx.edu./disk/bear-b/users/xxww/PolitiX/Dataset/'\n",
    "df_clean_tweet = pd.read_csv(path + 'PolitiX_Final_Tweet_Dataset.csv',  dtype = 'str', lineterminator = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b86c726-35e3-4d32-8297-1f189fcc9f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean_tweet\n",
    "df['full_text'] = df['full_text'].fillna(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e500d6-faed-4e60-b360-d2ad02f1452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_Retweet'] = df['text'].str.startswith(\"RT @\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd81279-bc8f-466b-8e9d-6ff2129b0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df.loc[df.is_Retweet == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bb8137-d58a-4336-b8aa-8d7901f0e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet = df_tweet.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d51e5f-2f03-4acf-9032-7c7af03a0083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(\"&\", \"\", regex=False)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(r\"\\bRT\\b\", \"\", regex=True)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(r\"&amp;\", \"\", regex=True)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(\"\\n\", \" \", regex=False)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(\"#\", \"\", regex=False)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(\"*\", \"\", regex=False)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(\"¶\", \"\", regex=False)\n",
    "    df.loc[:, 'clean_tweet'] = df['clean_tweet'].str.replace(r'[:;.,!&\\-_$/?\\'‘’%★“”\"]', \"\", regex=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_emoji(string):\n",
    "    # Use the 'emoji' library to remove emojis\n",
    "    clean_string = emoji.replace_emoji(string, replace='')  # Replaces emojis with empty string\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return clean_string.strip()  # Remove any leading/trailing spaces\n",
    "\n",
    "\n",
    "#Message Clean Function\n",
    "def msg_clean(msg):\n",
    "    #Remove URL\n",
    "    msg = re.sub(r'https?://\\S+|www\\.\\S+', \" \", msg)\n",
    "\n",
    "    #Remove Mentions\n",
    "    msg = re.sub(r'@\\w+',' ',msg)\n",
    "\n",
    "    #Remove HTML tags\n",
    "    msg = re.sub('r<.*?>',' ', msg)\n",
    "    msg = re.sub(r'\\d+', ' ', msg)\n",
    "    \n",
    "    #Remove Emoji from text\n",
    "    msg = remove_emoji(msg)\n",
    "\n",
    "\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0532828a-a30e-4c27-b650-83cd4f529fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['tweet_text'] = df_tweet['full_text']\n",
    "df_tweet.loc[:, 'clean_tweet'] = df_tweet['tweet_text'].astype(str).apply(lambda x: msg_clean(x))\n",
    "df_tweet = preprocess_text(df_tweet)\n",
    "df_tweet.loc[:, 'clean_tweet'] = df_tweet['clean_tweet'].str.replace(r'\\s+', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eab04a6-f29c-4fc1-98d5-15caa7e5c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_spaces = df_tweet['clean_tweet'].str.strip().eq(\"\")\n",
    "df_tweet = df_tweet[~empty_spaces].reset_index(drop = True)\n",
    "df_tweet = df_tweet.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76d8fed1-a2a1-4b8b-929f-54eeb926d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_tweet[['Tweet_ID','User_ID', 'clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05019baf-ca8f-4f62-affd-bd08a0d7985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute sentiment probabilities\n",
    "def get_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Get predicted label\n",
    "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    return probs[0], probs[1], probs[2], labels[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "178d6df1-7381-45ff-add1-2ff0ce7fe427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Tweet_ID  Negative   Neutral  Positive Predicted_Sentiment\n",
      "0  1829797265059324134  0.006210  0.967773  0.026199             Neutral\n",
      "1  1829708822392492046  0.036774  0.478027  0.485352            Positive\n",
      "2  1829395023932105078  0.196289  0.457275  0.346436             Neutral\n",
      "3  1829364778994680274  0.031281  0.418701  0.549805            Positive\n",
      "4  1829339189969563784  0.005066  0.930176  0.064697             Neutral\n",
      "Execution Time: 1671.4514 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Sentiment labels\n",
    "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Increase batch size to maximize GPU utilization\n",
    "BATCH_SIZE = 256  # Adjust based on your GPU memory\n",
    "\n",
    "# Specify device (GPU 3)\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Custom Dataset to tokenize on-the-fly\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, tokenizer, max_length=256):\n",
    "        self.tweets = tweets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(\n",
    "            self.tweets[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {key: val.squeeze(0) for key, val in encodings.items()}\n",
    "\n",
    "# Create dataset & dataloader\n",
    "dataset = TweetDataset(df_small[\"clean_tweet\"].tolist(), tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Run inference in large batches with mixed-precision\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = {key: val.to(device, non_blocking=True) for key, val in batch.items()}\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_preds.append(preds)\n",
    "\n",
    "# Convert list of numpy arrays to a single numpy array\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_preds = np.concatenate(all_preds)\n",
    "\n",
    "# Create final DataFrame\n",
    "df_sentiment = pd.DataFrame(all_probs, columns=['Negative', 'Neutral', 'Positive'])\n",
    "df_sentiment[\"Predicted_Sentiment\"] = np.array(labels)[all_preds]\n",
    "df_sentiment.insert(0, \"Tweet_ID\", df_small[\"Tweet_ID\"].values)\n",
    "\n",
    "# Display result\n",
    "print(df_sentiment.head())\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time: {end_time - start_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ab9f7d9-1c33-4ba2-9392-a2729f8a9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.to_csv('/a/bear.cs.xxx.edu./disk/bear-b/users/xxww/PolitiX/Dataset/All_Tweet_Sentiment.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6ff6d00-e02d-4a8b-9a23-137020c73fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.to_csv('/a/bear.cs.xxx.edu./disk/bear-b/users/xxww/PolitiX/Dataset/All_Tweet_Clean_Text.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d42af476-33c7-4fc2-813a-1ed5a2a10512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652707f9-d40c-4864-94be-839bc18c1f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
