{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d1826-8609-4425-adb0-91c00769993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install vllm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f13aa-6094-4c86-ab26-f10aa89629b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db32810-44ee-4159-888a-3cd5909ef776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d7785-621f-4ec1-87b9-724eb346bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797896d1-9ab4-4ae8-843c-902de5bdc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistral_common\n",
    "print(mistral_common.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3261c5-848e-46a3-96bf-70f698b45a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6361efa-c2a4-4f44-9670-0c90c937525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117ba57-4a3c-435c-b43a-10d644ae8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": device}\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391384a-e2e1-4655-b627-c246a4f91076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"transformers==4.36.2\" \"tokenizers==0.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022a5f4-92e8-4056-9992-900f70df4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = \"<s>[INST] can you find narrative in a tweet? [/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10cb8c-5c7a-4e1d-8e5d-5026bd522faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_generic_tweet(tweet):\n",
    "    prompt = f\"\"\"\n",
    "Determine if the tweet is \"Generic\" or \"Not Generic\".\n",
    "\n",
    "Definitions:\n",
    "- Generic: Conversation, automated, greeting, typical non-informative conversation. Examples include greetings, daily musings, automated messages.\n",
    "- Not Generic: Anything other than generic \n",
    "\n",
    "Examples:\n",
    "Tweet: \"Happy New Year!\" → Generic\n",
    "Tweet: \"What a day!\" → Generic\n",
    "Tweet: \"Earthquake hits Turkey, 6.1 magnitude.\" → Not Generic\n",
    "Tweet: \"Donald Trump will be the president of US.\" → Not Generic\n",
    "Tweet: \"Biden to address the nation at 8 PM.\" → Not Generic\n",
    "Tweet: \"Just posted a story on Instagram!\" → Generic\n",
    "Tweet: \"Follow me I will follow you back\" → Not Generic\n",
    "Tweet: \"20 accounts followed me.\"  → Generic\n",
    "Tweet: \"Do you remember when you joined Twitter I do MyTwitterAnniversary'\" → Generic\n",
    "Tweet: \"What do you think\" → Generic\n",
    "\n",
    "Tweet: \"{tweet}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=5,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"Answer:\")[-1].strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5273e-c468-47c1-b2ef-d2c4fbf258d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"TOM LEONARD: A chilling message in Manhattan \"\n",
    "print(\"Classification:\", is_generic_tweet(tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f458e-f122-4772-bf69-cd9fa6769fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.read_csv('All_Text_Duplicate_Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a6599-09f8-4154-a2e3-d11f405c5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_merge.drop_duplicates('Cluster_ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82348-bad8-49e9-8c47-22c85292f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterID = df_unique.Cluster_ID.tolist()\n",
    "clusterText = df_unique.clean_tweet.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37486bd-8f6e-4f0c-b448-f6e6f71a0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_generic = []\n",
    "for i in clusterText:\n",
    "    result_generic.append(is_generic_tweet(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f8946-21b6-47f6-90c9-1e9ce1a81ac6",
   "metadata": {},
   "source": [
    "# Narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a53b2-8f4b-4bbb-a405-ceb3f4cc265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_narrative(tweet):\n",
    "    prompt = f\"\"\"\n",
    "Evaluate the following tweet. If it contains a narrative, describe the narrative in 1–2 sentences. \n",
    "If it does not contain any narrative, respond with \"No narrative\".\n",
    "\n",
    "Definition:\n",
    "- Narrative = A tweet that promotes a belief, message, or claim — often expressing ideology, blame, cause-effect, or group identity.\n",
    "\n",
    "Examples:\n",
    "Tweet: \"The West is trying to destabilize us!\"\n",
    "Narrative: The tweet promotes the belief that Western countries are intentionally undermining another country’s stability.\n",
    "\n",
    "Tweet: \"Can't wait for the weekend!\"\n",
    "Narrative: No narrative\n",
    "\n",
    "Tweet: \"Vaccines are just a way for big pharma to profit off fear.\"\n",
    "Narrative: The tweet spreads a narrative that vaccine campaigns are driven by financial motives rather than public health.\n",
    "\n",
    "Tweet: \"{tweet}\"\n",
    "Narrative:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    result = decoded.split(\"Narrative:\")[-1].strip()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b54e8ff-824e-48e7-8ccc-ff8522ab5eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0023a-0328-434f-a9eb-53377faef01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_narrative(tweet):\n",
    "    prompt = f\"\"\"\n",
    "Determine if the following tweet contains a narrative.\n",
    "\n",
    "Definition:\n",
    "- A narrative expresses a **clear viewpoint, belief, or cause-effect claim**, especially if it is ideological, persuasive, or emotionally charged.\n",
    "- Generic or observational statements (e.g., \"Let's see what happens\", \"This is wild\", \"Wow\") are **not** narratives.\n",
    "\n",
    "Examples:\n",
    "Tweet: \"The West is trying to destabilize us!\"\n",
    "Narrative: The tweet promotes the belief that Western countries are intentionally undermining another country’s stability.\n",
    "\n",
    "Tweet: \"Can't wait for the weekend!\"\n",
    "Narrative: No narrative\n",
    "\n",
    "Tweet: \"Vaccines are just a way for big pharma to profit off fear.\"\n",
    "Narrative: The tweet spreads a narrative that vaccine campaigns are driven by financial motives rather than public health.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Answer \"Yes\" or \"No\" to indicate if the tweet contains a narrative.\n",
    "2. If \"Yes\", explain the narrative in 1–2 sentences.\n",
    "3. If \"No\", say \"No narrative present\".\n",
    "\n",
    "Tweet: \"{tweet}\"\n",
    "Contains narrative?\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa58bde-8402-451f-8937-c3c33f3b777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Black niggers are the worst.They do nothing but steal\"\n",
    "out = detect_narrative(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029b4e9-6aa9-486d-a0ac-d6c41720f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f623916-7cc7-44f5-a762-4352be2cece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_narrative_output(output_text):\n",
    "    match_contains = re.findall(r'Contains narrative\\?\\s*(Yes|No)', output_text, re.IGNORECASE)\n",
    "    contains = match_contains[-1].strip() if match_contains else \"No\"\n",
    "    if contains.lower() == \"yes\":\n",
    "        match_narrative = re.findall(r'Contains narrative\\?\\s*Yes\\.?\\s*(?:The narrative is)?\\s*(.*)', output_text, re.IGNORECASE | re.DOTALL)\n",
    "        narrative = match_narrative[-1].strip() if match_narrative else \"\"\n",
    "    else:\n",
    "        narrative = \"\"\n",
    "\n",
    "    return contains, narrative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c40ce-1a46-44c3-86bb-607094615c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_output = detect_narrative(text)\n",
    "contains, narrative = parse_narrative_output(raw_output)\n",
    "print(contains)  \n",
    "print(narrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5d59a-5be7-467d-83a4-c14b68808e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af441a53-52d4-49f7-a337-e648ace2388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in clusterText:\n",
    "    result_generic.append(is_generic_tweet(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6178a-df23-4e33-adef-44f140308da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr_present = []\n",
    "narr_text = []\n",
    "k = 0\n",
    "for i in clusterText:\n",
    "    raw_output = detect_narrative(i)\n",
    "    contains, narrative = parse_narrative_output(raw_output)\n",
    "    narr_present.append(contains)\n",
    "    narr_text.append(narrative)\n",
    "    k = k+ 1\n",
    "    if k ==10:\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901be43-b204-4e02-8dca-06acec25f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "narr_present = []\n",
    "narr_text = []\n",
    "for i in clusterText:\n",
    "    raw_output = detect_narrative(i)\n",
    "    contains, narrative = parse_narrative_output(raw_output)\n",
    "    narr_present.append(contains)\n",
    "    narr_text.append(narrative)\n",
    "    \n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Start time: {start_time}\")\n",
    "print(f\"End time: {end_time}\")\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c57b44-8c18-43a6-b1f7-774aa6ec52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(narr_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510fbf8-4f61-47a9-822d-93b3f61d40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterID = df_unique.Cluster_ID.tolist()\n",
    "clusterText = df_unique.clean_tweet.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aac1f6-d6bf-48b9-b754-3d145ce3dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narr_data = pd.DataFrame({\n",
    "    'Cluster_ID': clusterID,\n",
    "    'Tweet_Text': clusterText,\n",
    "    'Narrative_Present': narr_present,\n",
    "    'Narrative': narr_text\n",
    "    \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139232aa-a7cd-45bc-9859-c0cf8fd37c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd5d37-f325-417a-b8d5-fa7316692591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d13dee-a41e-48fa-8163-354e2081cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_narr_data.to_csv('Narrative_Generated.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3d724-655e-4e5e-934e-58a8d1d16620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
