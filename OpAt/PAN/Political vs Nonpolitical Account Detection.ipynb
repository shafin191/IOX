{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d545ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d46144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_tweets = pd.read_csv('Non_Political_Account_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info= pd.read_csv('Non_Pol_Manual_Check.csv', dtype = 'str')\n",
    "df_non_pol_account_info2 = pd.read_csv('NonPol_Manual_Label2.csv', dtype = 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info2.loc[df_non_pol_account_info2.User_ID == '1.61596E+18', 'User_ID'] = '1615962057249280000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee0f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69150f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info2['User_ID'] =df_non_pol_account_info2['User_ID'].astype('str')\n",
    "#df_non_pol_account_info['User_ID'] =df_non_pol_account_info['User_ID'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0231b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_pol_account_info2.loc[df_non_pol_account_info2.User_ID == '1.61596E+18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.61596E+18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info2.loc[df_non_pol_account_info2.User_ID == '1.61596E+18', 'User_ID'] = '1615962057249280000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info2['User_ID'] =df_non_pol_account_info2['User_ID'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_account_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdabed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol = df_non_pol_account_info[df_non_pol_account_info.User_ID.isin(df_non_pol_account_info2.loc[df_non_pol_account_info2.Label == 'NonPolitical'].User_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761427fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol =df_non_pol.drop_duplicates(subset = ['User_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol2 = pd.read_csv('Training_Political_Affiliation_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06423f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol2.loc[df_non_pol2.Party == 'No'].User_ID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_pol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e24e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account = pd.read_csv('All_Fake_without_debunkers_Updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac00587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol3= df_account[df_account.User_ID.isin(df_non_pol2.loc[df_non_pol2.Party == 'No'].User_ID.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol3_short = df_non_pol3[['User_ID', 'UserName', 'ScreenName','UserDescription']].rename(columns = {'UserDescription': 'User_Description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebe181",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_non_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol3_short['User_ID'] = df_non_pol3_short['User_ID'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8113e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_pol3_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_new = pd.concat([df_non_pol, df_non_pol3_short])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed09bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol= df_account[df_account.User_ID.isin(df_non_pol2.loc[df_non_pol2.Party != 'No'].User_ID.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5fa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol_check = df_pol.dropna(subset = ['UserDescription']).sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa42f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol_check= df_pol_check[['User_ID', 'UserName', 'ScreenName','UserDescription']].rename(columns = {'UserDescription': 'User_Description'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_new['Label'] = 0\n",
    "df_pol_check['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pol_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95950949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_non_pol_new, df_pol_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8445e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f62377",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d587421",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['ScreenName'] = df_data['ScreenName'].str.replace('@', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01072d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data[['User_ID', 'User_Description','UserName','ScreenName', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stop words if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Example DataFrame\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "df_data['User_Description_Updated'] = df_data['User_Description'].apply(remove_stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec7b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ded8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['User_Description_Updated'] = df_data['User_Description_Updated'].replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_links(text):\n",
    "    return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "\n",
    "# Apply the function to remove links from the 'Text' column\n",
    "df_data['User_Description_Updated'] = df_data['User_Description_Updated'].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3307570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_data['User_Description_Updated'] = df_data['User_Description_Updated'].fillna('')\n",
    "#df['text_column'] = df['text_column'].fillna('')\n",
    "\n",
    "# Initialize TfidfVectorizer with bi-gram and 50 top features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=50)\n",
    "\n",
    "# Fit and transform the text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_data['User_Description_Updated'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "#print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_df['Label'] = df_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714a99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2895a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, tfidf_df, df_data['Label'], cv=k_fold, scoring='f1')\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean_f1 = cv_scores.mean()\n",
    "print(\"Mean F1 Score:\", mean_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1141060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Example DataFrame with class labels and text data\n",
    "# data = {'text': [\"This is an example sentence for class 0\",\n",
    "#                  \"Another example sentence for class 0\",\n",
    "#                  \"Yet another sentence for class 1\",\n",
    "#                  \"Example sentence for class 1\",\n",
    "#                  \"An example sentence for class 1\"],\n",
    "#         'class': [0, 0, 1, 1, 1]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Function to extract bigrams from text\n",
    "def extract_bigrams(text):\n",
    "    words = text.split()\n",
    "    return [words[i] + \" \" + words[i+1] for i in range(len(words)-1)]\n",
    "\n",
    "# Select top 25 instances of each class\n",
    "top_25_class_0 = df_data.loc[df_data['Label'] == 0]\n",
    "top_25_class_1 = df_data.loc[df_data['Label'] == 1]\n",
    "\n",
    "# Extract bigrams for each class\n",
    "class_0_bigrams = [extract_bigrams(text) for text in top_25_class_0['User_Description_Updated']]\n",
    "class_1_bigrams = [extract_bigrams(text) for text in top_25_class_1['User_Description_Updated']]\n",
    "\n",
    "# Count occurrences of bigrams in each class\n",
    "class_0_bigram_counts = Counter([bg for sublist in class_0_bigrams for bg in sublist])\n",
    "class_1_bigram_counts = Counter([bg for sublist in class_1_bigrams for bg in sublist])\n",
    "\n",
    "# Calculate significance score (Sbiдr am) for each bigram\n",
    "significance_scores = {}\n",
    "for bigram, count in class_1_bigram_counts.items():\n",
    "    if bigram in class_0_bigram_counts:\n",
    "        significance_scores[bigram] = math.log(count / class_0_bigram_counts[bigram])\n",
    "    else:\n",
    "        significance_scores[bigram] = math.log(count)\n",
    "\n",
    "# Display the significance scores\n",
    "print(\"Significance Scores (Sbiдr am):\")\n",
    "for bigram, score in significance_scores.items():\n",
    "    print(f\"{bigram}: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#significance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b926f0",
   "metadata": {},
   "source": [
    "# User Tweet Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_political_tweets = pd.read_csv('Non_Political_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_political_tweets.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bef83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c26f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tweet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all_tweet.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_tweet = df_all_tweet[df_all_tweet.User_ID.isin(df_data.loc[df_data.Label== 1].User_ID.astype('int').tolist())]\n",
    "df_check_tweet2 = df_all_tweet[df_all_tweet.User_ID.isin(df_data.loc[df_data.Label== 0].User_ID.astype('int').tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffca34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_tweet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25731d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.loc[df_data.Label== 1].User_ID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check_tweet.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c1920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_check_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_non_political_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tweets(group):\n",
    "    if len(group) >= 20:\n",
    "        return group.sample(20)\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "subset_df = df_check_tweet.groupby('User_ID', group_keys=False).apply(sample_tweets)\n",
    "subset_df2 = df_check_tweet2.groupby('User_ID', group_keys=False).apply(sample_tweets)\n",
    "#print(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0786a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to remove mentions and links from tweet text\n",
    "def remove_mentions_links(text):\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove links\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply the function to the tweet_text column\n",
    "subset_df['clean_tweet_text'] = subset_df['Tweet_Text'].apply(remove_mentions_links)\n",
    "subset_df2['clean_tweet_text'] = subset_df2['Tweet_Text'].apply(remove_mentions_links)\n",
    "df_non_political_tweets['clean_tweet_text'] = df_non_political_tweets['text'].apply(remove_mentions_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f27f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_political_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tweets(group):\n",
    "    return ' '.join(group)\n",
    "\n",
    "# Merge tweets for each user\n",
    "merged_tweets_df_non_political = df_non_political_tweets.groupby('User_ID')['clean_tweet_text'].agg(merge_tweets).reset_index()\n",
    "merged_tweets_df_political = subset_df.groupby('User_ID')['clean_tweet_text'].agg(merge_tweets).reset_index()\n",
    "merged_tweets_df_non_political2 = subset_df2.groupby('User_ID')['clean_tweet_text'].agg(merge_tweets).reset_index()\n",
    "#print(merged_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f08bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_non_political2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5656f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = nltk.corpus.stopwords.words('hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stopwords_hindi import hindi_sw\n",
    "sw = hindi_sw.get_hindi_sw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e76c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e96be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc640718",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9548563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Additional stop words to be removed\n",
    "extra_stop_words = ['RT', 'ji', 'amp']\n",
    "\n",
    "# Function to remove stop words from tweet text\n",
    "def remove_stopwords(text):\n",
    "    text = text.replace('RT :', '')\n",
    "    stop_words_english = stopwords.words('english')\n",
    "\n",
    "    # Get the list of Hindi stop words\n",
    "    stop_words_hindi = list(sw)\n",
    "\n",
    "    # Combine both English and Hindi stop words\n",
    "    stop_words_combined = stop_words_english + stop_words_hindi\n",
    "    stop_words = set(stop_words_combined)\n",
    "    # Add extra stop words\n",
    "    stop_words.update(extra_stop_words)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Apply the function to the tweet_text column\n",
    "merged_tweets_df_non_political['clean_tweet_text'] = merged_tweets_df_non_political['clean_tweet_text'].apply(remove_stopwords)\n",
    "merged_tweets_df_non_political2['clean_tweet_text'] = merged_tweets_df_non_political2['clean_tweet_text'].apply(remove_stopwords)\n",
    "merged_tweets_df_political['clean_tweet_text'] = merged_tweets_df_political['clean_tweet_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bc8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def tweet_to_base_form(tweet):\n",
    "    # Convert tweet text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Tokenize the tweet into words\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Initialize WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Lemmatize each word and join them back into a string\n",
    "    base_form_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    base_form_tweet = ' '.join(base_form_words)\n",
    "    \n",
    "    return base_form_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef632f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_non_political['clean_tweet_text'] = merged_tweets_df_non_political['clean_tweet_text'].apply(tweet_to_base_form)\n",
    "merged_tweets_df_political['clean_tweet_text'] = merged_tweets_df_political['clean_tweet_text'].apply(tweet_to_base_form)\n",
    "\n",
    "merged_tweets_df_non_political2['clean_tweet_text'] = merged_tweets_df_non_political2['clean_tweet_text'].apply(tweet_to_base_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_non_political['clean_tweet_text'] = merged_tweets_df_non_political['clean_tweet_text'].apply(remove_stopwords)\n",
    "merged_tweets_df_non_political2['clean_tweet_text'] = merged_tweets_df_non_political2['clean_tweet_text'].apply(remove_stopwords)\n",
    "merged_tweets_df_political['clean_tweet_text'] = merged_tweets_df_political['clean_tweet_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_political['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets_df_non_political['Label'] =0\n",
    "merged_tweets_df_non_political2['Label'] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fe47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_tweet = pd.concat([merged_tweets_df_political[['User_ID', 'clean_tweet_text', 'Label']],merged_tweets_df_non_political[['User_ID', 'clean_tweet_text', 'Label']], merged_tweets_df_non_political2[['User_ID', 'clean_tweet_text', 'Label']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8531a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_tweet=df_data_tweet.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data_tweet.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = ['bjp', 'inc', 'modi', 'rahul', 'gandhi', 'yogi', 'congress', 'aap', 'kejriwal', \n",
    "              'भाजपा','भारतीय राष्ट्रीय कांग्रेस', 'मोदी' ,'राहुल','गांधी' ,'योगी','कांग्रेस' ,'आप', 'केजरीवाल' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_data['User_Description_Updated'] = df_data['User_Description_Updated'].fillna('')\n",
    "#df['text_column'] = df['text_column'].fillna('')\n",
    "\n",
    "# Initialize TfidfVectorizer with bi-gram and 50 top features\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=25)\n",
    "\n",
    "# Fit and transform the text column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_data['User_Description_Updated'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "#print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_tweet['clean_tweet_text'] = df_data_tweet['clean_tweet_text'].fillna('')\n",
    "\n",
    "# Initialize TfidfVectorizer with bi-gram and 50 top features\n",
    "tfidf_vectorizer_tweet = TfidfVectorizer(ngram_range=(1, 3), max_features=25)\n",
    "\n",
    "# Fit and transform the text column\n",
    "tfidf_matrix_tweet = tfidf_vectorizer_tweet.fit_transform(df_data_tweet['clean_tweet_text'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names_tweet = tfidf_vectorizer_tweet.get_feature_names_out()\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df_tweet = pd.DataFrame(tfidf_matrix_tweet.toarray(), columns=feature_names_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa868e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ec19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(feature_names_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9884762",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['User_ID'] = df_data['User_ID'].astype('str')\n",
    "tfidf_df_tweet['User_ID'] = df_data_tweet['User_ID'].astype('str')\n",
    "df_data_small = df_data[['User_ID', 'Label']]\n",
    "df_data_small['User_ID'] = df_data_small['User_ID'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = pd.merge(tfidf_df, tfidf_df_tweet, on = 'User_ID', how = 'left')\n",
    "tf_idf_data = pd.merge(tf_idf_data, df_data_small[['User_ID', 'Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8278343",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = tf_idf_data.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b733d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data.User_ID = tf_idf_data.User_ID.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca24662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account = pd.read_csv('All_Fake_without_debunkers_Updated.csv')\n",
    "df_non_account = pd.read_csv('Non_Political_Info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account[['User_ID', 'AveragePosts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef267d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.to_datetime(df_non_account['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431743a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp.now(tz='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_account['date'] = pd.to_datetime(df_non_account['Created_At'])\n",
    "df_non_account['date'] = df_non_account['date'].dt.strftime('%b %d, %Y')\n",
    "df_non_account['date'] = pd.to_datetime(df_non_account['date']).dt.tz_localize('UTC')\n",
    "df_non_account['Account_Age'] = (pd.Timestamp.now(tz='UTC') - df_non_account['date']).dt.days\n",
    "df_non_account['AveragePosts'] = df_non_account.Post_Count/ df_non_account.Account_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e206111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f46a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_account['AveragePosts'] = df_non_account.Post_Count/ df_non_account.Account_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_account[['User_ID','AveragePosts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ff446",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_small_check = pd.concat([df_account[['User_ID', 'AveragePosts']], df_non_account[['User_ID','AveragePosts']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_small_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15223222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = pd.merge(tf_idf_data, df_acc_small_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ec2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = tf_idf_data['Label']\n",
    "train_data = tf_idf_data.drop(columns = ['User_ID', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f206df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class_label = tf_idf_data['Label']\n",
    "train_data = tf_idf_data.drop(columns = ['User_ID', 'Label'])\n",
    "\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_classifier2 = RandomForestClassifier()\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "k_fold2 = KFold(n_splits=10)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores2_f1 = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='f1')\n",
    "cv_scores2_pr = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='precision')\n",
    "cv_scores2_re = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='recall')\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean2_f1 = cv_scores2_f1.mean()\n",
    "print(\"Mean F1 Score:\", mean2_f1)\n",
    "print(\"Mean PR Score:\", cv_scores2_pr.mean())\n",
    "print(\"Mean RE Score:\",cv_scores2_re.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_data.groupby('Label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229eb0ce",
   "metadata": {},
   "source": [
    "# Account Political Affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier2.fit(train_data, class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf_classifier2.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': train_data.columns, 'Importance': feature_importances})\n",
    "feature_importance_df.sort_values('Importance', ascending = False).iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0537d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_following = pd.read_csv('Non_Political_Following.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33816ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niva_duck = pd.read_csv('NivaDuck.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3295ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_niva_duck_small = df_niva_duck[['id', 'party']].rename(columns = {'id': 'Following_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0ce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_follow_pol = pd.merge(df_non_pol_following, df_niva_duck_small, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57fa570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_non_pol_follow_pol['Political'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_follow_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9643463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_follow_pol.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22444d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Group by 'User ID' and 'Political Label' and count occurrences\n",
    "grouped = df_non_pol_follow_pol.groupby(['User_ID', 'party']).size().reset_index(name='Frequency')\n",
    "\n",
    "# Pivot the table to have User ID as index and Political Label as columns with frequency counts\n",
    "pivot_table = grouped.pivot_table(index='User_ID', columns='party', values='Frequency', fill_value=0)\n",
    "\n",
    "# If a user doesn't have a particular label, fill it with 0\n",
    "pivot_table = pivot_table.reindex(columns=['AAP', 'BJP', 'INC', 'Other'], fill_value=0)\n",
    "\n",
    "#print(pivot_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_pol_follow_pol = df_non_pol_follow_pol.groupby('User_ID').count().reset_index()[['User_ID', 'Following_ID']].rename(columns = {'Following_ID': 'Total_Following'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9fc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_new = pd.merge(pivot_table.reset_index(), df_non_pol_follow_pol, how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ec28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_new['Political'] = pivot_new['AAP'] + pivot_new['BJP'] + pivot_new['INC'] + pivot_new['Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b18b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_new['Political_Ratio'] = pivot_new['Political']/pivot_new['Total_Following']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8694af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_new = pivot_new.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c551cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_new[['User_ID', 'Political_Ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f8be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_political_following = pd.read_csv('Cluster_Following_Data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ba813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_following_info_new = pd.concat([df_political_following[['User_ID', 'Political_Ratio']], pivot_new[['User_ID', 'Political_Ratio', 'AAP', 'BJP', 'INC', 'Other']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ba6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = pd.merge(tf_idf_data, df_following_info_new, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_political_following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data = tf_idf_data.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data[['User_ID', 'Political_Ratio', 'AAP', 'BJP', 'INC','Other']].to_csv('Cluster_Following_Data3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a73ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class_label = tf_idf_data['Label']\n",
    "train_data = tf_idf_data.drop(columns = ['User_ID', 'Label'])\n",
    "\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_classifier2 = RandomForestClassifier()\n",
    "\n",
    "# Define 10-fold cross-validation\n",
    "k_fold2 = KFold(n_splits=10)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores2_f1 = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='f1')\n",
    "cv_scores2_pr = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='precision')\n",
    "cv_scores2_re = cross_val_score(rf_classifier2, train_data, class_label, cv=k_fold2, scoring='recall')\n",
    "\n",
    "# Calculate mean accuracy\n",
    "mean2_f1 = cv_scores2_f1.mean()\n",
    "print(\"Mean F1 Score:\", mean2_f1)\n",
    "print(\"Mean PR Score:\", cv_scores2_pr.mean())\n",
    "print(\"Mean RE Score:\",cv_scores2_re.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_data.to_csv('Political_NonPolitical_Model_Data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e621b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# political_labels_up = {\n",
    "#     'AAP': 2,\n",
    "#     'ABVP': 1,\n",
    "#     'BJP': 1,\n",
    "#     'DMK': 2,\n",
    "#     'GOV' : 2,\n",
    "#     'INC' : 0,\n",
    "#     'No' : 2,\n",
    "#     'RJD' : 0,\n",
    "#     'SP': 0,\n",
    "#     'RSS' : 1,\n",
    "#     'SS' : 1,\n",
    "#     'GOV':2,\n",
    "#     'Undetermined' : 2,\n",
    "#     'VHP': 1\n",
    "# }\n",
    "\n",
    "\n",
    "political_labels_up = {\n",
    "    'AAP': 2,\n",
    "    'ABVP': 1,\n",
    "    'BJP': 1,\n",
    "    'DMK': 2,\n",
    "    'GOV' : 2,\n",
    "    'INC' : 0,\n",
    "    'No' : 3,\n",
    "    'RJD' : 0,\n",
    "    'SP': 0,\n",
    "    'RSS' : 1,\n",
    "    'SS' : 1,\n",
    "    'GOV':2,\n",
    "    'Undetermined' : 3,\n",
    "    'VHP': 1\n",
    "}\n",
    "\n",
    "\n",
    "y = tf_idf_data['Label']\n",
    "X = tf_idf_data.drop(columns = ['User_ID', 'Label'])\n",
    "\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "# df_new_short6= df_new_short6.reset_index(drop = True)\n",
    "# # Split the data into features (X) and labels (y)\n",
    "# X = df_new_short6.drop(columns=['Party_Label', 'Party', 'User_ID', 'UserFollowers', \n",
    "#                                 'UserFriends', 'UserFavoriteCount', 'TotalStatus',\n",
    "#                                 'AccountAgeinDays', 'AveragePosts', 'AverageFavoriteCount',\n",
    "#                                 '%_of_Retweet_Collected', '%_of_Tweet_Collected', 'Total_Hashtags',\n",
    "#                                 'Retweet_Count', 'Favorite_Count', 'Average_Engagement_on_Tweets',\n",
    "#                                 'Average_Engagement_on_Retweets', 'Tweet_Max_Engagement',\n",
    "#                                 'Tweet_Max_Retweet', 'Tweet_Max_Favorite', 'Tweet_Max_Hashtags',\n",
    "#                                 'h-index', 'Total_Unique_Source', 'followers_friends_ratio',\n",
    "#                                 'tweet_retweet_ratio', 'h-index_Total'])\n",
    "\n",
    "# y = df_new_short6['Party_Label']\n",
    "\n",
    "#X2 = (X - X.min()) / (X.max() - X.min())\n",
    "\n",
    "# Define a logistic regression model\n",
    "model =  RandomForestClassifier()\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring=scorer)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# Calculate and print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac687f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have loaded your dataset into X and y\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "\n",
    "\n",
    "# Now X_train_resampled and y_train_resampled contain the oversampled data\n",
    "\n",
    "\n",
    "# Assuming you have defined X (features) and y (labels) and have imported necessary modules\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# Initialize lists to store predictions and true labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    # Perform oversampling using SMOTE\n",
    "    #smote = SMOTE( random_state=42)\n",
    "    #X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model =  RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    precision = precision_score(y_test, preds, average='weighted')\n",
    "    recall = recall_score(y_test, preds, average='weighted')\n",
    "    # Here, you would typically use your fine-tuned model to make predictions on X_test\n",
    "    # For demonstration, let's assume we have random predictions\n",
    "    #preds = np.random.choice([0, 1], size=len(y_test))  # Replace this with your model predictions\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    # Print evaluation metrics for this fold\n",
    "    print(\"Fold F1 Score:\", f1)\n",
    "    print(\"Fold Precision:\", precision)\n",
    "    print(\"Fold Recall:\", recall)\n",
    "    \n",
    "\n",
    "    # Append predictions and true labels\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(y_test)\n",
    "\n",
    "# Compute confusion matrix for all predictions and true labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "\n",
    "# Print mean values of evaluation metrics\n",
    "print(\"Mean F1 Score:\", mean_f1)\n",
    "print(\"Mean Precision:\", mean_precision)\n",
    "print(\"Mean Recall:\", mean_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the confusion matrix to numpy array\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "confusion_matrix = np.array(conf_matrix)\n",
    "\n",
    "true_positives = np.diag(confusion_matrix)\n",
    "false_positives = np.sum(confusion_matrix, axis=0) - true_positives\n",
    "false_negatives = np.sum(confusion_matrix, axis=1) - true_positives\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print precision, recall, and F1 score for each class\n",
    "for i in range(len(precision)):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Precision: {precision[i]}\")\n",
    "    print(f\"  Recall: {recall[i]}\")\n",
    "    print(f\"  F1 Score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b35d93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
