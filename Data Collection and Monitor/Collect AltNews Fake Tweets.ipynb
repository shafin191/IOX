{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ba1d5-b603-4347-a4d3-7e9160f7041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import twikit\n",
    "import random\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "from twikit import Client\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f2348-7e9f-45c5-bd8b-94584d574b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "current_date = now.date()\n",
    "client = Client(\"en-US\")\n",
    "print(twikit.__version__)\n",
    "\n",
    "#Generate Cookie for X account and use the cookie for the account\n",
    "client.load_cookies('V:\\\\SBERT All Embedding\\\\cookie_JohnFisher.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1bbfa-eed8-4ff2-9037-a2eb5a594be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a74b3-140e-494f-93bd-89c9c7efbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#await client.search_tweet('python', 'Latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098cc65-bca5-493c-8ef1-1d0c35747297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_new = pd.read_csv('Potential_Data_Look.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f00c37-0268-40f0-a0de-c1b8c255ced2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_read_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f15f3-3c09-419a-be82-2af611addeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_read_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba7e97-96d0-4e65-8f53-007ea2dd5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_new2 = df_read_new.drop_duplicates('Cleaned Link').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3952ff-8c58-43ca-aaaf-aee1746b2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_new2['Tweet_ID'] = df_read_new2['Cleaned Link'].str.split('/').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102be917-1295-4200-937a-7d0c502661a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_new2['Tweet_ID'] = df_read_new2['Tweet_ID'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb4d84-3b4e-431c-a230-8208b0231d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sus_tweet = df_read_new2['Tweet_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fab32-ac14-48b2-8fbd-194617971767",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sus_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26911e06-5d6a-42e1-b85d-2cdc28434e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_store = []\n",
    "user_to_store = []\n",
    "check_num = 1\n",
    "tweet_prob = []\n",
    "for i in all_sus_tweet:\n",
    "    tweets_to_store = []\n",
    "    user_to_store = []\n",
    "    \n",
    "    \n",
    "    time.sleep(60)\n",
    "    \n",
    "    if check_num%100 == 0:\n",
    "        time.sleep(60)\n",
    "    \n",
    "    try:\n",
    "        t = await client.get_tweet_by_id(i)\n",
    "    except:\n",
    "        print('ID Problem: ' + str(i))\n",
    "        tweet_prob.append(i)\n",
    "        continue\n",
    "        \n",
    "    # Stroe User Information    \n",
    "    user_to_store.append({\n",
    "        'User_ID':t.user.id,\n",
    "        'ScreenName':t.user.screen_name,\n",
    "        'Created_At': t.user.created_at,\n",
    "        'UserName':t.user.name,\n",
    "        'Default_Image':t.user.default_profile_image,\n",
    "        'User_Description':t.user.description, \n",
    "        'User_Description_URL':t.user.description_urls, \n",
    "        'Fast_Follower_Count':t.user.fast_followers_count,\n",
    "        'Normal_Follower_Count':t.user.normal_followers_count,\n",
    "        'Favorite_Count':t.user.favourites_count, \n",
    "        'Follower_Count':t.user.followers_count, \n",
    "        'Friend_Count':t.user.following_count,\n",
    "        'Total_Status_Count':t.user.statuses_count,\n",
    "        'Custom_Timeline':t.user.has_custom_timelines,\n",
    "        'Blue_Verified':t.user.is_blue_verified,\n",
    "        'Veified':t.user.verified,\n",
    "        'Is_Translator':t.user.is_translator,\n",
    "        'Listed_Count':t.user.listed_count,\n",
    "        'User_Location':t.user.location,\n",
    "        'Media_Count':t.user.media_count,        \n",
    "        'Pinned_Tweet_ID':t.user.pinned_tweet_ids,\n",
    "        'Sensitivity':t.user.possibly_sensitive,\n",
    "        'Profile_Banner_Link':t.user.profile_banner_url,\n",
    "        'Profile_Image_Link':t.user.profile_image_url,\n",
    "        'Is_Protected':t.user.protected,\n",
    "        'Post_Count':t.user.statuses_count,\n",
    "        'Translatory_Type':t.user.translator_type,\n",
    "        'Banned_In_Countries':t.user.withheld_in_countries,\n",
    "        'URL':t.user.url,\n",
    "        'URL2':t.user.urls})\n",
    "\n",
    "    with open(\"V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\New_Fake_Tweets\\\\Account\\\\\" + \"USER_\"+ str(t.user.id)+\"_\"+str(check_num) +\".pkl\", \"wb\") as f:\n",
    "        pickle.dump(user_to_store, f)\n",
    "            \n",
    "    try:\n",
    "        df_user = pd.DataFrame(user_to_store)\n",
    "        df_user.to_csv(\"V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\New_Fake_Tweets\\\\Account\\\\\" +\n",
    "                       \"USER_\"+ str(t.user.id)+\"_\"+str(check_num) +\".csv\", index = False)\n",
    "    except:\n",
    "        print('Problem')\n",
    "        \n",
    "            \n",
    "    # Store Tweet Information    \n",
    "    if tweet.text[0:4] == 'RT @':\n",
    "        try:\n",
    "            tweets_to_store.append({\n",
    "                    'User_ID': t.user.id,\n",
    "                    'Tweet_ID': t.id,\n",
    "                    'Retweet_ID': t.retweeted_tweet.id,\n",
    "                    'Retweet_User_ID': t.retweeted_tweet.user.id,\n",
    "                    'created_at': t.created_at,\n",
    "                    'Retweet_created_at':t.retweeted_tweet.created_at,\n",
    "                    'favorite_count': t.favorite_count,\n",
    "                    'retweet_count': t.retweet_count,\n",
    "                    'media':t.media,\n",
    "                    'Retweet_Media': t.retweeted_tweet.media,\n",
    "                    'text': t.text,\n",
    "                    'Retweet_Text': t.retweeted_tweet.text,\n",
    "                    'full_text': t.full_text,\n",
    "                    'Text_Lang': t.lang,\n",
    "                    'Retweet_Lang':t.retweeted_tweet.lang,\n",
    "                    'View_Count': t.view_count,\n",
    "                    'Comment_Count':t.reply_count,\n",
    "                    'Quote_Count': t.quote_count,\n",
    "                    'Text_Hashtags':t.hashtags,\n",
    "                    'has_community_note': t.has_community_notes, \n",
    "                    'Link': t.urls,       \n",
    "                    'Data_Collection_Time': datetime.datetime.now()\n",
    "                \n",
    "                    })\n",
    "                \n",
    "        except:\n",
    "                tweets_to_store.append({\n",
    "                    'User_ID': t.user.id,\n",
    "                    'Tweet_ID': t.id,\n",
    "                    'created_at': t.created_at,\n",
    "                    'favorite_count': t.favorite_count,\n",
    "                    'retweet_count': t.retweet_count,\n",
    "                    'media':t.media,\n",
    "                    'text': t.text,\n",
    "                    'full_text': t.full_text,\n",
    "                    'Text_Lang': t.lang,\n",
    "                    'View_Count': t.view_count,\n",
    "                    'Comment_Count':t.reply_count,\n",
    "                    'Quote_Count': t.quote_count,\n",
    "                    'Text_Hashtags':t.hashtags,\n",
    "                    'has_community_note': t.has_community_notes, \n",
    "                    'Link': t.urls,       \n",
    "                    'Data_Collection_Time': datetime.datetime.now()})\n",
    "                \n",
    "    else:\n",
    "        tweets_to_store.append({\n",
    "                    'User_ID': t.user.id,\n",
    "                    'Tweet_ID': t.id,\n",
    "                    'created_at': t.created_at,\n",
    "                    'favorite_count': t.favorite_count,\n",
    "                    'retweet_count': t.retweet_count,\n",
    "                    'media':t.media,\n",
    "                    'text': t.text,\n",
    "                    'full_text': t.full_text,\n",
    "                    'Text_Lang': t.lang,\n",
    "                    'View_Count': t.view_count,\n",
    "                    'Comment_Count':t.reply_count,\n",
    "                    'Quote_Count': t.quote_count,\n",
    "                    'Text_Hashtags':t.hashtags,\n",
    "                    'has_community_note': t.has_community_notes, \n",
    "                    'Link': t.urls,       \n",
    "                    'Data_Collection_Time': datetime.datetime.now()})\n",
    "        \n",
    "    with open(\"V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\New_Fake_Tweets\\\\Tweets\\\\\" + \"Tweet_\"+ str(t.id)+ \"_\" + str(check_num) +\".pkl\", \"wb\") as f:\n",
    "            pickle.dump(tweets_to_store, f)\n",
    "    try:\n",
    "        df_tweet = pd.DataFrame(tweets_to_store)\n",
    "        df_tweet.to_csv(\"V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\New_Fake_Tweets\\\\Tweets\\\\\" +str(t.id) +\"_\" + str(check_num) +\"_Tweets.csv\", index = False)\n",
    "\n",
    "    except:\n",
    "        print('Problem')\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "    check_num = check_num + 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total Problem: \" + str(len(tweet_prob)))\n",
    "\n",
    "with open(\"Problem_Tweets.txt\", \"w\") as f:\n",
    "    for item in tweet_prob:\n",
    "        f.write(str(item) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240860a-400f-4fc5-91eb-e61e7c84bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
