{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af36995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import tweepy\n",
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6792d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_7\\*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96442158",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d3d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d334c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_process(alltweets):\n",
    "    tweet_id = [tweet.id_str for tweet in alltweets]\n",
    "    tweet_created = [tweet.created_at for tweet in alltweets]\n",
    "    tweet_text = [tweet.full_text for tweet in alltweets]\n",
    "    tweet_hashtags = [tweet.entities['hashtags'] for tweet in alltweets]\n",
    "    user_id = [tweet.user.id for tweet in alltweets]\n",
    "    screen_name_df = [tweet.user.screen_name for tweet in alltweets]\n",
    "    retweet_count =  [tweet.retweet_count for tweet in alltweets]\n",
    "    favorite_count = [tweet.favorite_count for tweet in alltweets]\n",
    "    lang = [tweet.lang for tweet in alltweets]\n",
    "    source = [tweet.source for tweet in alltweets]\n",
    "\n",
    "\n",
    "    retweet_text = []\n",
    "    i=0\n",
    "    for i in range(len(alltweets)):\n",
    "\n",
    "        try:\n",
    "            retweet_text.append(alltweets[i].retweeted_status.full_text) \n",
    "        except:\n",
    "            retweet_text.append(None)\n",
    "        i = i+1\n",
    "\n",
    "\n",
    "    mod_tweet_hashtags = []\n",
    "    total_hashtags = []\n",
    "\n",
    "    for t in tweet_hashtags:\n",
    "        tag = \"\"\n",
    "        count = 0\n",
    "        if (bool(t) == False):\n",
    "            mod_tweet_hashtags.append('None')\n",
    "            total_hashtags.append(count)\n",
    "        else:\n",
    "            for i in t:\n",
    "                tag = tag + \" \" + i.get('text')\n",
    "                count +=1\n",
    "            mod_tweet_hashtags.append(tag)\n",
    "            total_hashtags.append(count)\n",
    "\n",
    "    mention = [tweet.entities['user_mentions'] for tweet in alltweets]\n",
    "    mod_tweet_mention_sc = []\n",
    "    mod_tweet_mention_id = []\n",
    "    k = 0\n",
    "    for t in mention:\n",
    "        if (bool(t) == False):\n",
    "            mod_tweet_mention_sc.append('None')\n",
    "            mod_tweet_mention_id.append('None')\n",
    "        else:\n",
    "            mod_tweet_mention_sc.append(mention[k][0]['screen_name'])\n",
    "            mod_tweet_mention_id.append(mention[k][0]['id_str'])\n",
    "        k = k+1\n",
    "\n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Text': retweet_text,\n",
    "                  'Tweet_HashTags': mod_tweet_hashtags,\n",
    "                  'Total_Hashtags' : total_hashtags,\n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Retweet_Screen_Name':mod_tweet_mention_sc,\n",
    "                  'Retweet_Account_ID':mod_tweet_mention_id,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "\n",
    "    dfr = pd.DataFrame(tweet_dict) \n",
    "    dfr['Retweet_Source_ScreenName'] = 'None'\n",
    "    dfr['Retweet'] = False\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0, len(dfr)):\n",
    "        if 'RT' == dfr['Tweet_Text'][i].split()[0]:\n",
    "            try:\n",
    "                dfr['Retweet_Source_ScreenName'][i] = dfr['Tweet_Text'][i].split()[1][1:-1]\n",
    "                dfr['Retweet'][i] = True\n",
    "            except:\n",
    "                print(i)\n",
    "                print(dfr['Tweet_Text'][i])\n",
    "                dfr['Retweet'][i] = True\n",
    "\n",
    "\n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01ab6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = tweet_process(objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "347441ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda88b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63bef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cfb4573",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc8c2fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9d00bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9a9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('Fake_List_7_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a2ab6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_16\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_16_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec26c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_17\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_17_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5b0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46d1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af92312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_10\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_10_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66fb03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_11\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_11_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62d8e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_12\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_12_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0b252a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_13\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_13_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e41d6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_14\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_14_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff581665",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Fake Accounts\\Fake_List_15\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96df98a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = tweet_process(objects[0])\n",
    "for i in range(1, len(objects)):\n",
    "    temp_data = tweet_process(objects[i])\n",
    "    frames = [all_data, temp_data]\n",
    "    all_data = pd.concat(frames)\n",
    "all_data = all_data.reset_index(drop = True)\n",
    "all_data.to_csv('Fake_List_15_AccountTweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1084a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aefa677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('531 Fake Account Tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c9d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e96eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['User_ScreenName'] == 'SirPareshRawal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bff187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.User_ScreenName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bf36bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
