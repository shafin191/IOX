{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import time\n",
    "from ttictoc import tic,toc\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249716b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('200 Benign Accounts.csv')\n",
    "df2 = pd.read_csv('400 benign account.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Benign_Account_138.data', 'rb') as filehandle:\n",
    "        x= pickle.load( filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9cb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-1].user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ede508",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = [df, df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.concat(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa759ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = dfx.drop_duplicates('TwitterScreenName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = [df, dfx]\n",
    "dfx = pd.concat(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbac56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = dfx.drop_duplicates(subset='TwitterScreenName', keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1800d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name = df['TwitterScreenName']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name=screen_name[160:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name = df['TwitterScreenName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c56399",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name = screen_name.drop_duplicates(keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25867e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eda097",
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = \"\"\n",
    "key2 = \"\"\n",
    "key3 = \"\"\n",
    "key4 = \"\"\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(key1, key2)\n",
    "auth.set_access_token(key3, key4)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a416c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "old_length = 0\n",
    "new_length =0\n",
    "consumer_key = key1\n",
    "consumer_secret = key2\n",
    "access_key = key3\n",
    "access_secret = key4\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "for i in screen_name:\n",
    "    old_length = len(alltweets)\n",
    "    try:\n",
    "        new_tweets = api.user_timeline(screen_name = i,count=200, tweet_mode=\"extended\")\n",
    "\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        while len(new_tweets) > 0:\n",
    "\n",
    "            new_tweets = api.user_timeline(screen_name = i,count=200,max_id=oldest, tweet_mode=\"extended\")\n",
    "\n",
    "            alltweets.extend(new_tweets)\n",
    "\n",
    "            oldest = alltweets[-1].id - 1\n",
    "            with open('Benign_Account_'+str(it) +'.data', 'wb') as filehandle:\n",
    "                pickle.dump(alltweets, filehandle)\n",
    "\n",
    "        new_length = len(alltweets) - old_length\n",
    "        print(f\"...{new_length} tweets downloaded so far for {i}\")\n",
    "        time.sleep(15*30)\n",
    "        it+=1\n",
    "        \n",
    "    except:\n",
    "        print(\"Account Not Found: \" + str(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, retry_count=10, retry_delay=300, retry_errors=set([503]))\n",
    "\n",
    "for accountvar in screen_name:\n",
    "    try:\n",
    "        followerids =[]\n",
    "\n",
    "        for user in tweepy.Cursor(api.followers_ids, screen_name=accountvar,count=5000).items():\n",
    "            followerids.append(user)\n",
    "            time.sleep(.0002)\n",
    "        print (len(followerids))\n",
    "\n",
    "        with open('U:/Twitter Research/Fake News Scraping/Fake News Account Followers Data/Follower_ID' \n",
    "                  + str(accountvar) + '.data', 'wb') as filehandle:\n",
    "            pickle.dump(followerids, filehandle)\n",
    "\n",
    "        print(toc())\n",
    "    except:\n",
    "        print(str(accountvar) + \" not in Twitter.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6b6bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for accountvar in screen_name:\n",
    "    print (accountvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa51816",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True, retry_count=10, retry_delay=300, retry_errors=set([503]))\n",
    "followerids =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic()\n",
    "accountvar = ''\n",
    "followerids =[]\n",
    "\n",
    "for user in tweepy.Cursor(api.get_follower_ids, screen_name=accountvar,count=5000).items():\n",
    "    followerids.append(user)\n",
    "    time.sleep(.0002)\n",
    "print (len(followerids))\n",
    "\n",
    "with open('U:/Twitter Research/Fake News Scraping/Fake News Account Followers Data/Follower_ID/' \n",
    "          + str(accountvar) + '.data', 'wb') as filehandle:\n",
    "    pickle.dump(followerids, filehandle)\n",
    "\n",
    "print(toc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(followerids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "userids = followerids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True, retry_count=10, retry_delay=300, retry_errors=set([503]))\n",
    "\n",
    "\n",
    "accountvar = \"\"\n",
    "tic()\n",
    "def get_usernames(userids, api):\n",
    "    fullusers = []\n",
    "    u_count = len(userids)\n",
    "    print(u_count)\n",
    "    try:\n",
    "        for i in range(int(u_count/100) + 1):            \n",
    "            end_loc = min((i + 1) * 100, u_count)\n",
    "            fullusers.extend(\n",
    "                api.lookup_users(user_id=userids[i * 100:end_loc])                \n",
    "            )\n",
    "            if(len(fullusers) % 20000 < 99):\n",
    "                print(len(fullusers) % 20000)\n",
    "                print(len(fullusers))\n",
    "                time.sleep(150)\n",
    "        return fullusers\n",
    "    except:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print ('Something went wrong, quitting...')\n",
    "\n",
    "fullusers = get_usernames(followerids,api)\n",
    "print(toc())\n",
    "\n",
    "\n",
    "with open('U:/Twitter Research/Fake News Scraping/Fake News Account Followers Data/Follower_Info' \n",
    "                  + str(accountvar) + '.data', 'wb') as filehandle:\n",
    "    pickle.dump(fullusers, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f08e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fullusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4239ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link = pd.read_excel('Fake Tweet Link.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0be8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link = df_link.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80686b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ad70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_id(link):\n",
    "    return str(link).split('/')[-1]\n",
    "\n",
    "df_link['Tweet_ID'] = df_link['Link'].apply(link_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_link(id):\n",
    "    res = True\n",
    "    if (len(id) < 5) or id =='Post Deleted':\n",
    "        res = False\n",
    "    \n",
    "    return res\n",
    "\n",
    "df_link['Valid_Link'] = df_link['Tweet_ID'].apply(valid_link)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a08271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535efd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link.loc[df_link.Valid_Link ==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dcbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link.loc[df_link.Valid_Link == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ef0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for index, row in df_link.iterrows():\n",
    "    if(row['Valid_Link'] == True):\n",
    "        try:\n",
    "            rt = api.get_retweets(row['Tweet_ID'], count = 100)\n",
    "            print(len(rt))\n",
    "            with open('U:/Twitter Research/Fake News Scraping/Fake News Tweet Sharing/RT_UPDATED/RT_' \n",
    "                  + str(row['Tweet_ID']) + '.data', 'wb') as filehandle:\n",
    "                pickle.dump(rt, filehandle)\n",
    "            time.sleep(60)\n",
    "        except:\n",
    "            print(str(row['Tweet_ID']) + ' -  tweet got deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fav = client.get_liking_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebfc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for index, row in df_link.iterrows():\n",
    "    if(row['Valid_Link'] == True):\n",
    "        try:\n",
    "            fav = client.get_liking_users(row['Tweet_ID'])\n",
    "            fav_id =[] \n",
    "            i=0\n",
    "            for i in fav.data:\n",
    "                fav_id.append(i.id)\n",
    "            with open('U:/Twitter Research/Fake News Scraping/Fake News Tweet Sharing/LIKE/LIKE_' \n",
    "                      + str(row['Tweet_ID']) + '.data', 'wb') as filehandle:\n",
    "                pickle.dump(fav_id, filehandle)\n",
    "            time.sleep(60)\n",
    "            print('Success for : ' + str(row['Tweet_ID']))\n",
    "        except:\n",
    "            print(str(row['Tweet_ID']) + ' -  tweet got deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fav.data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your_data.json', 'w') as out:\n",
    "    json.dump(fav.data[0].id,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a8667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
