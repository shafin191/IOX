{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import getpass\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e8126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40365abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2336da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ba773",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.altnews.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41126d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca71bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, 1080)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87173b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "a = driver.find_element(by=By.XPATH, value=\"(//div[@class='pbs-col col__xs-1_2'])[position()=574]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e09d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCROLL_PAUSE_TIME = 10\n",
    "k = 1\n",
    "\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    k = k + 1\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        time.sleep(SCROLL_PAUSE_TIME * 2)\n",
    "    if k == 300:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc44881",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07730c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0ee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_link = []\n",
    "for i in range(1,575):\n",
    "    a = driver.find_element(by=By.XPATH, value=\"(//div[@class='pbs-col col__xs-1_2'])[position()=\" + str(i) +\"]\")\n",
    "    all_link.append(a.find_element(By.CSS_SELECTOR, 'a').get_attribute('href'))\n",
    "\n",
    "textfile = open(\"all_link_altNews_2024_updated.txt\", \"w\")\n",
    "for element in all_link:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import pickle\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = input(\"Enter the text file name: \")\n",
    "with open(str(file_name)+'.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_alt=[]\n",
    "for x in lines:\n",
    "    url_alt.append(x.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for URL in url_alt[0:1]:\n",
    "    \n",
    "    try:\n",
    "        page = requests.get(URL)\n",
    "\n",
    "    except:\n",
    "        print(\"Page not found. 404 Error\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    topic = soup.find(id=\"breadcrumbs\")\n",
    "    results = soup.find(id=\"primary\")\n",
    "    news_time = soup.find(class_=\"page-header-inner p-h-i\")\n",
    "    news_text = soup.find(\"content-area c-a\")\n",
    "    job_elements = results.find_all(\"blockquote\", class_=\"twitter-tweet\")\n",
    "\n",
    "    \n",
    "    target = soup.find('h4')\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==\"blockquote\":\n",
    "            job_elements.append(sib)\n",
    "    \n",
    "    for i in range(len(job_elements)):\n",
    "        text = job_elements[i].find(\"p\").text\n",
    "        x = job_elements[i].select(\"p:nth-of-type(2)\")\n",
    "        screen_name = re.findall(r'\\(.*?\\)', str(x))\n",
    "        screen_name = str(screen_name)\n",
    "        screen_name = screen_name.strip(\"[]\")\n",
    "        screen_name = screen_name.replace(\"(\",\"\").replace(\")\",\"\").replace(\"'\",\"\")\n",
    "\n",
    "        try:\n",
    "            link = str(job_elements[i].select(\"a\")[-1])\n",
    "            link = link[link.find(\"=\")+2:link.find(\"?\")]\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "for URL in url_alt[4:6]:\n",
    "    try:\n",
    "        page = requests.get(URL, headers=headers)\n",
    "        page.raise_for_status()  # Ensure no 400 or 500 errors\n",
    "        print(page.content)  # Proceed with your logic\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "for URL in url_alt[100:102]:\n",
    "    try:\n",
    "        page = session.get(URL, headers=headers)\n",
    "        page.raise_for_status()  # Ensure no 400 or 500 errors\n",
    "        print(page.content)  # Proceed with your logic\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b486fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(URL, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29268d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "proxies = {\n",
    "    'http': 'http://103.230.199.17:10000'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "url = \"https://www.altnews.in/haldwani-violence-video-of-scuffle-in-mp-falsely-shared-with-communal-slurs/\"  # Replace with the actual URL\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    response.raise_for_status()\n",
    "    print(response.text)  # Output the content\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b555c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.altnews.in/shimla-sp-same-date-of-birth-on-aadhaar-cards-result-of-default-auto-fill-rejects-propaganda-false-narrative/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111be6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212255be",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_alt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aec6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url_alt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b93c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2233347",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c098779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"U:/Twitter Research/Fake News Scraping/AltNews_New_Data/1.html\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ad73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(url_alt)):\n",
    "    driver.get(url_alt[i])\n",
    "    with open(\"U:/Twitter Research/Fake News Scraping/AltNews_New_Data/\" + str(i) + \".html\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(driver.page_source)\n",
    "    time.sleep(20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_alt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f823e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_metadata_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    link_tag = soup.find('link', rel='canonical')\n",
    "    link = link_tag['href'] if link_tag else \"Link not found\"\n",
    "\n",
    "    title_tag = soup.find('meta', property='og:title')\n",
    "    title = title_tag['content'] if title_tag else \"Title not found\"\n",
    "\n",
    "    published_tag = soup.find('meta', property='article:published_time')\n",
    "    published_date = published_tag['content'] if published_tag else \"Published date not found\"\n",
    "\n",
    "    return link, title, published_date\n",
    "\n",
    "directory_path = 'V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\All Webpages\\\\'\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".html\") or filename.endswith(\".htm\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "        link, title, published_date = extract_metadata_from_html(html_content)\n",
    "\n",
    "        data.append((filename, link, title, published_date))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['File', 'Link', 'Title', 'Published Date'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv('extracted_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d370a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_metadata_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    link_tag = soup.find('link', rel='canonical')\n",
    "    link = link_tag['href'] if link_tag else \"Link not found\"\n",
    "\n",
    "    title_tag = soup.find('meta', property='og:title')\n",
    "    title = title_tag['content'] if title_tag else \"Title not found\"\n",
    "\n",
    "    published_tag = soup.find('meta', property='article:published_time')\n",
    "    published_date = published_tag['content'] if published_tag else \"Published date not found\"\n",
    "\n",
    "    breadcrumb = soup.find('nav', class_='breadcrumb-nav yoast-breadcrumb')\n",
    "    if breadcrumb:\n",
    "        breadcrumb_links = breadcrumb.find_all('a')\n",
    "        topic = breadcrumb_links[-1].get_text() if breadcrumb_links else \"Topic not found\"\n",
    "    else:\n",
    "        topic = \"Topic not found\"\n",
    "\n",
    "    return link, title, published_date, topic\n",
    "\n",
    "directory_path = 'V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\All Webpages\\\\'\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".html\") or filename.endswith(\".htm\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        link, title, published_date, topic = extract_metadata_from_html(html_content)\n",
    "\n",
    "        data.append((filename, link, title, published_date, topic))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['File', 'Link', 'Title', 'Published Date', 'Topic'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv('extracted_metadata_with_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Topic').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('V:\\\\SBERT All Embedding\\\\AltNews_New_Data\\\\New_Fake_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a435df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.groupby('Text_Lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53bb098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4e5b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
