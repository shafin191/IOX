{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b32866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import tweepy\n",
    "import pickle\n",
    "import glob\n",
    "import tweepy\n",
    "import datetime\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25c5d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor1_Tweets_Dec_17\\*.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5880128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bd0ac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdeb321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f54ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28ceeaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ddc726a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db22e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c47d07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a435575",
   "metadata": {},
   "outputs": [],
   "source": [
    "db3.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor1_Tweets_Dec_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f55c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor1_Tweets_Dec_19\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db4 = db\n",
    "db4.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor1_Tweets_Dec_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d7d0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor1_Tweets_Dec_20\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db5 = db\n",
    "db5.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor1_Tweets_Dec_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25dcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3d6447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db6 = db\n",
    "db6.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor_Multi_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2fc90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets_Dec_14\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db7 = db\n",
    "db7.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor_Multi_Tweets_Dec_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b29b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets_Dec_17\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db8 = db\n",
    "db8.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor_Multi_Tweets_Dec_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f7511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dca7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets_Dec_19\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db9 = db\n",
    "db9.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor_Multi_Tweets_Dec_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9978b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets_Dec_20\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n",
    "                \n",
    "def tweet_process(alltweets):\n",
    "    tweet_id = alltweets.id_str\n",
    "    tweet_created = alltweets.created_at\n",
    "    \n",
    "    try:\n",
    "        tweet_text = alltweets.full_text\n",
    "    except:\n",
    "        tweet_text = alltweets.text\n",
    "    try:\n",
    "        tweet_media = alltweets.entities['media'][0]['expanded_url']\n",
    "    except:\n",
    "        tweet_media = 'No Media'\n",
    "        \n",
    "    user_id = alltweets.user.id \n",
    "    screen_name_df = alltweets.user.screen_name \n",
    "    retweet_count =  alltweets.retweet_count \n",
    "    favorite_count = alltweets.favorite_count \n",
    "    lang = alltweets.lang \n",
    "    source = alltweets.source  \n",
    "    \n",
    "    tweet_dict = {'Tweet_ID': tweet_id, \n",
    "                  'Tweet_Time': tweet_created, \n",
    "                  'Tweet_Text': tweet_text, \n",
    "                  'Retweet_Count': retweet_count,\n",
    "                  'Favorite_Count': favorite_count,\n",
    "                  'Tweet_Source' : source,\n",
    "                  'Tweet_Lang': lang,\n",
    "                  'Tweet_Media': tweet_media,\n",
    "                  'User_ID': user_id,\n",
    "                  'User_ScreenName': screen_name_df\n",
    "                  }  \n",
    "\n",
    "    return tweet_dict\n",
    "\n",
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db, xx]\n",
    "        db = pd.concat(frames)\n",
    "        k = k+1\n",
    "db10 = db\n",
    "db10.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\CSV Files\\Monitor_Multi_Tweets_Dec_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fffe68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.concat([db1, db2, db3, db4, db5, db6, db7, db8,db9, db10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "34f21137",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e761a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_acc = db.groupby('User_ID').count().reset_index(drop = False)[['User_ID', 'Tweet_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "576eadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cf719b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique = db.drop_duplicates(subset=['Tweet_ID'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c3c26639",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique = db_unique.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3aca44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(db_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb4f66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "db_unique['Date'] = db_unique['Tweet_Time'].dt.date\n",
    "db_unique['Date_Diff'] = date.today() - db_unique['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "623002d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique_last_2_months = db_unique.loc[db_unique.Date_Diff < '60 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "2da1b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique_last_2_months.drop_duplicates('Tweet_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "5ed1c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique_last_2_months.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "694dd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_unique.groupby('User_ID').count().reset_index()[['User_ID', 'Tweet_ID']].to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Account_List_From_Posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b7d1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_multi_tweet = db_unique.groupby('Tweet_Text').count().sort_values(by = 'Tweet_ID',ascending = False).reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "abe45497",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_multi_tweet = db_multi_tweet.loc[db_multi_tweet.Tweet_ID > 1][['Tweet_Text', 'Tweet_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4cfd439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_multi_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4396b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_all = db.loc[db.Tweet_Text == db_multi_tweet.Tweet_Text[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafa79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e0c42d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1693):\n",
    "    db1_dup_new = db.loc[db.Tweet_Text == db_multi_tweet.Tweet_Text[i]]\n",
    "    frames = [db_duplicate_all, db1_dup_new]\n",
    "\n",
    "    db_duplicate_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5faa819a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_duplicate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e31b3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_unique = db_duplicate_all.drop_duplicates(['Tweet_Text', 'User_ID'], keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c222f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_text = db_duplicate_unique.groupby('Tweet_Text').count().sort_values(by = 'Tweet_ID', ascending = False).reset_index(drop = False)[0:358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37ff2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_text_text = db_text.Tweet_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bad59a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_text_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3fc58f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_unique.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d6ec519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_unique.groupby('Tweet_Text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "392f2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new = db_duplicate_unique.loc[db_duplicate_unique.Tweet_Text == db_text_text[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "20844d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,358):\n",
    "    db_duplicate_new1 = db_duplicate_unique.loc[db_duplicate_unique.Tweet_Text == db_text_text[i]]\n",
    "    frames = [db_duplicate_new, db_duplicate_new1]\n",
    "\n",
    "    db_duplicate_new = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "782c9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new.groupby('Tweet_Text').count().reset_index(drop = False).sort_values(by = 'Tweet_ID', ascending = False)[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "27216cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_acc = db_duplicate_new.groupby('User_ID').count().reset_index(drop = False).sort_values(by = 'Tweet_ID', ascending = False)[['User_ID','Tweet_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6a018928",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_acc.to_csv('U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Duplicate_Post.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d3bb0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "75f37017",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new['Date'] = db_duplicate_new['Tweet_Time'].dt.date\n",
    "db_duplicate_new['Date_Diff'] = date.today() - db_duplicate_new['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "869ccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new_last_2_month = db_duplicate_new.loc[db_duplicate_new.Date_Diff < '60 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3e538a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_new_last_2_month.groupby('Tweet_Text').count()[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0aa8ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\All Acc\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "94309fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_data(fullusers):\n",
    "    user_id=[]\n",
    "    user_name=[]\n",
    "    user_screenName=[]\n",
    "    user_location=[]\n",
    "    user_description=[]\n",
    "    user_url=[]\n",
    "    user_followersCount=[]\n",
    "    user_friendsCount=[]\n",
    "    user_createdAt=[]\n",
    "    user_favCount=[]\n",
    "    user_verified=[]\n",
    "    user_statusCount=[]\n",
    "    user_language=[]\n",
    "    userLast_status = []\n",
    "    user_protected = []\n",
    "    user_following_screenName = []\n",
    "    user_following_userID = []\n",
    "    user_profile_image = []\n",
    "    user_profile_banner = []\n",
    "    for i in range(len(fullusers)):\n",
    "        if (fullusers[i].protected==False):\n",
    "            user_id.append(fullusers[i].id)\n",
    "            user_name.append(fullusers[i].name)\n",
    "            user_screenName.append(fullusers[i].screen_name)\n",
    "            user_location.append(fullusers[i].location)\n",
    "            user_description.append(fullusers[i].description)\n",
    "            user_url.append(fullusers[i].url)\n",
    "            user_followersCount.append(fullusers[i].followers_count)\n",
    "            user_friendsCount.append(fullusers[i].friends_count)\n",
    "            user_createdAt.append(fullusers[i].created_at)\n",
    "            user_favCount.append(fullusers[i].favourites_count)\n",
    "            user_verified.append(fullusers[i].verified)\n",
    "            user_statusCount.append(fullusers[i].statuses_count)\n",
    "            user_language.append(fullusers[i].lang)\n",
    "            user_protected.append(fullusers[i].protected)\n",
    "            user_profile_image.append(fullusers[i].profile_image_url)\n",
    "            try:\n",
    "                userLast_status.append(fullusers[i].status.created_at)\n",
    "            except:\n",
    "                userLast_status.append(None)\n",
    "                \n",
    "        if (fullusers[i].protected==True):\n",
    "            user_id.append(fullusers[i].id)\n",
    "            user_name.append(fullusers[i].name)\n",
    "            user_screenName.append(fullusers[i].screen_name)\n",
    "            user_location.append(fullusers[i].location)\n",
    "            user_description.append(fullusers[i].description)\n",
    "            user_url.append(fullusers[i].url)\n",
    "            user_followersCount.append(fullusers[i].followers_count)\n",
    "            user_friendsCount.append(fullusers[i].friends_count)\n",
    "            user_createdAt.append(fullusers[i].created_at)\n",
    "            user_favCount.append(fullusers[i].favourites_count)\n",
    "            user_verified.append(fullusers[i].verified)\n",
    "            user_statusCount.append(fullusers[i].statuses_count)\n",
    "            user_language.append(fullusers[i].lang)\n",
    "            user_protected.append(fullusers[i].protected)\n",
    "            user_profile_image.append(fullusers[i].profile_image_url)\n",
    "            try:\n",
    "                userLast_status.append(fullusers[i].status.created_at)\n",
    "            except:\n",
    "                userLast_status.append(None)\n",
    "\n",
    "    user_info = pd.DataFrame(\n",
    "        {'User ID': user_id,\n",
    "         'UserName': user_name,\n",
    "         'Screen Name': user_screenName,\n",
    "         'UserLocation': user_location,\n",
    "         'UserDescription': user_description,\n",
    "         'UserURL': user_url,\n",
    "         'UserFollowers':user_followersCount,\n",
    "         'UserFriends': user_friendsCount,\n",
    "         'UserCreated': user_createdAt,\n",
    "         'UserFavoriteCount': user_favCount,\n",
    "         'UserVerified': user_verified,\n",
    "         'TotalStatus': user_statusCount,\n",
    "         'UserLanguage':user_language,\n",
    "         'UserLastStatus': userLast_status,\n",
    "         'UserProtected': user_protected,\n",
    "         'UserProfileLink':user_profile_image,\n",
    "         \n",
    "        })\n",
    "    \n",
    "    return user_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "415a358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_accounts = user_data(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "eff87200",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db_accounts.loc[db_accouts.UserVerified == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8368f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_accounts.rename(columns={'User ID':'User_ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "772c15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ca886c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_duplicate_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e6a89321",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dup_acc_details = db_accounts.merge(db_duplicate_acc, on = 'User_ID', how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f66e3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dup_acc_details.groupby('UserVerified').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "37d952d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dup_acc_details.groupby('User_ID').count().sort_values(by = 'UserName', ascending = False)[0:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6d7e674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_acc.merge(db_accounts, on = 'User_ID').groupby('User_ID').count().sort_values(by='Tweet_ID', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "62f25100",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "37af0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_account_unique = db_accounts.groupby('User_ID').count().reset_index(drop = False)[['User_ID', 'UserName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dad6bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_accounts.drop_duplicates('User_ID').groupby('UserVerified').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e3b678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.sort_values('Tweet_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "797b0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbu = db.sort_values(by=['Tweet_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ebf05a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbu_last = dbu.drop_duplicates(subset=['User_ID'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c5996e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "dbu_last['Date'] = dbu_last['Tweet_Time'].dt.date\n",
    "dbu_last['Date_Diff'] = date.today() - dbu_last['Date']\n",
    "dbu_last.loc[dbu_last.Date_Diff > '30 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "30cc644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "dbu_last['Date_Diff'] = date.today() - dbu_last['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "139ea8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbu_last.loc[dbu_last.Date_Diff > '30 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7af97f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4febeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_latest = db.drop_duplicates(['Tweet_Text', 'User_ID'], keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b0d2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_latest.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False)[0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53e32e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.loc[db.Tweet_Text == \"Banks on Sale !\\nAirports on Sale !\\nRailways on Sale!\\nRoads on Sale!\\n\\nWhat's next ?\\n#IndiaOnSale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8faf7c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob(\"U:\\Twitter Research\\Fake News Scraping\\Monitoring Accounts\\Monitor_Multi_Tweets\\*.data\")\n",
    "objects = []\n",
    "user = []\n",
    "j=0\n",
    "for i in file:\n",
    "    j=j+1\n",
    "    with (open(i, \"rb\")) as openfile:\n",
    "        user.append(i)\n",
    "        while True:\n",
    "            try:\n",
    "                objects.append(pickle.load(openfile))\n",
    "            except EOFError:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "558c6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt= tweet_process(objects[0][0])\n",
    "xx =pd.DataFrame(tt,index=[0])\n",
    "db1=xx\n",
    "k = 0\n",
    "for i in range(0, len(objects)):\n",
    "    if i == 0:\n",
    "        ta = 1\n",
    "    else:\n",
    "        ta = 0\n",
    "    for j in range(ta, len(objects[i])):\n",
    "        tt= tweet_process(objects[i][j])\n",
    "        xx =pd.DataFrame(tt,index=[k])\n",
    "        frames = [db1, xx]\n",
    "        db1 = pd.concat(frames)\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b0345946",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ccd13a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2adcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_latest = db1.drop_duplicates(['Tweet_Text', 'User_ID'], keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "72685aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_latest.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e3fa967",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_duplicate = db1_latest.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False)[0:82].reset_index(drop = False)[['Tweet_Text', 'Tweet_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "171a2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d383f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.loc[db1.Tweet_Text == db1_duplicate.Tweet_Text[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc7367a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_dup_new = db1.loc[db1.Tweet_Text == db1_duplicate.Tweet_Text[1]]\n",
    "frames = [db1_duplicate_all, db1_dup_new]\n",
    "\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "697d89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_duplicate_all = db1.loc[db1.Tweet_Text == db1_duplicate.Tweet_Text[0]]\n",
    "for i in range(1,82):\n",
    "    db1_dup_new = db1.loc[db1.Tweet_Text == db1_duplicate.Tweet_Text[i]]\n",
    "    frames = [db1_duplicate_all, db1_dup_new]\n",
    "\n",
    "    db1_duplicate_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bae3fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db1_duplicate_all.groupby('User_ScreenName').count().sort_values('Tweet_ID', ascending = False)[0:42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9f684eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [db, db1]\n",
    "db_total = pd.concat(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "27b6e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total = db_total.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6a9d9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total = db_total.drop_duplicates('Tweet_ID', keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "afd19654",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f3258fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_latest = db_total.drop_duplicates(['Tweet_Text', 'User_ID'], keep= 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "48138aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_latest.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False)[0:137].reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7baff9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "db_total_latest['Date'] = db_total_latest['Tweet_Time'].dt.date\n",
    "db_total_latest['Date_Diff'] = date.today() - db_total_latest['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3943328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_last_month = db_total_latest.loc[db_total_latest.Date_Diff < '42 days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "afceca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(db_total_last_month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bd8392c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_last_month.groupby('User_ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ffd6c88d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_total_last_month.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False).reset_index(drop = False)[0:95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bd439765",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_last_month_dup = db_total_last_month.groupby('Tweet_Text').count().sort_values('Tweet_ID', ascending = False).reset_index(drop = False)[0:95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a194de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_last_month.groupby('User_ID').count().sort_values('User_ID', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0a7522dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_last_month_dup.Tweet_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dc41f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_duplicate_all = db_total.loc[db_total.Tweet_Text == db_total_last_month_dup.Tweet_Text[0]]\n",
    "for i in range(1,95):\n",
    "    db_total_duplicate_new = db_total.loc[db_total.Tweet_Text == db_total_last_month_dup.Tweet_Text[i]]\n",
    "    frames = [db_total_duplicate_all, db_total_duplicate_new]\n",
    "\n",
    "    db_total_duplicate_all = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "249a2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_duplicate_all.groupby('User_ScreenName').count().sort_values('Tweet_ID', ascending = False)[0:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8e3b177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_total_duplicate_all[40:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
